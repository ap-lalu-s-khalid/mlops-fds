{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lalu/Documents/MLOps/mlops-fds/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import sys\n",
    "sys.path.append('../fds-model')  # Add the parent directory to the path\n",
    "import utils  # Import the utils module\n",
    "from utils import preprocess_data, save_model, load_config  # Assuming these functions exist in utils.py\n",
    "from dataset import get_data_from_bigquery  # Importing the function from dataset.py\n",
    "import model  # Assuming you have a model.py for your model's definition and training logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test, model_config):\n",
    "    results = []\n",
    "    for name, params in model_config.items():\n",
    "        logging.info(f'Training model: {name} with parameters: {params}')\n",
    "        \n",
    "        m = model.create_model(name, **params)\n",
    "        m.fit(X_train, y_train)\n",
    "        y_pred = m.predict(X_test)\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')  # Use average='binary' for binary classification\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, m.predict_proba(X_test)[:, 1])  # Assumes model has predict_proba method\n",
    "        except AttributeError:\n",
    "            roc_auc = None\n",
    "            logging.warning(f'Model {name} does not support ROC AUC calculation')\n",
    "        \n",
    "        # Log metrics\n",
    "        logging.info(f'Accuracy for {name}: {accuracy}')\n",
    "        logging.info(f'F1 Score for {name}: {f1}')\n",
    "        if roc_auc is not None:\n",
    "            logging.info(f'ROC AUC Score for {name}: {roc_auc}')\n",
    "        \n",
    "        # Save the results\n",
    "        results.append({\n",
    "            'model': name,\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc_score': roc_auc\n",
    "        })\n",
    "        save_model(m, name)  # Use utils.save_model to save the model\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    pd.DataFrame(results).to_csv('results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration (including BigQuery query and project ID)\n",
    "config = load_config(\"../fds-model/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset from BigQuery\n",
    "df = get_data_from_bigquery(config[\"bigquery\"][\"query\"], config[\"bigquery\"][\"project_id\"], config[\"bigquery\"][\"credentials_path\"])\n",
    "# Drop columns\n",
    "columns_drop = ['id', 'user_id', 'created_at', 'updated_at', 'blocked_at', 'expiry', 'payment_at', 'expiry_days']\n",
    "df = df.drop(columns=columns_drop, axis=1)\n",
    "target_column = 'fraud_status'\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (assuming this function expects a DataFrame and returns train/test splits)\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df, target_column=target_column, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "train_and_evaluate(X_train, X_test, y_train, y_test, config['models'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
